{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Simple tokenization\n",
        "\n",
        "import numpy as np\n",
        "doc=[\"The sunshine brightened the day.\",\n",
        "    \"The moonlight cast a gentle glow.\",\n",
        "    \"We reached the mountain peak at dawn.\",\n",
        "    \"The ocean breeze was refreshing.\",\n",
        "    \"The whispering winds carried secrets.\",\n",
        "    \"The starlit sky was mesmerizing.\",\n",
        "    \"We followed the forest trail.\",\n",
        "    \"The desert dune seemed endless.\",\n",
        "    \"The thunderstorm was intense.\",\n",
        "    \"We found the rainbow's end.\"]\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer=Tokenizer(oov_token='nothing')\n",
        "tokenizer.fit_on_texts(doc)\n",
        "\n",
        "tokenizer.word_index\n",
        "\n",
        "tokenizer.word_counts\n",
        "\n",
        "tokenizer.document_count\n",
        "\n",
        "sequences=tokenizer.texts_to_sequences(doc)\n",
        "sequences\n",
        "\n",
        "# Padding\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "sequences=pad_sequences(sequences,padding='post')\n",
        "sequences\n",
        "\n",
        "# Sentimental analysis\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,SimpleRNN,Embedding,Flatten\n",
        "(x_train,y_train),(x_test,y_test)=imdb.load_data()\n",
        "\n",
        "x_train # data is preprocessed\n",
        "\n",
        "# now applying padding as each review is not of same size\n",
        "\n",
        "x_train=pad_sequences(x_train,padding='post',maxlen=50)\n",
        "x_test=pad_sequences(x_test,padding='post',maxlen=50)\n",
        "\n",
        "# RNN model\n",
        "\n",
        "model=Sequential(SimpleRNN(32,input_shape=(50,1),return_sequences=False))  # in sentimental analysis it stays false but not as always\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "# model compilation\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(x_train,y_train,epochs=5,validation_data=(x_test,y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5hCTFpHPrtj",
        "outputId": "61ceefe1-0ce6-4270-ec21-0c969c701bba"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_4 (SimpleRNN)    (None, 32)                1088      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1121 (4.38 KB)\n",
            "Trainable params: 1121 (4.38 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 12s 14ms/step - loss: 0.6976 - accuracy: 0.5020 - val_loss: 0.6937 - val_accuracy: 0.5030\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.6933 - accuracy: 0.5065 - val_loss: 0.6952 - val_accuracy: 0.5036\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 0.6933 - accuracy: 0.5024 - val_loss: 0.6961 - val_accuracy: 0.5019\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 14s 17ms/step - loss: 0.6931 - accuracy: 0.5064 - val_loss: 0.6944 - val_accuracy: 0.5010\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 11s 15ms/step - loss: 0.6929 - accuracy: 0.5056 - val_loss: 0.6937 - val_accuracy: 0.5055\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ef7a8273b20>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZA1pq__Psc2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}