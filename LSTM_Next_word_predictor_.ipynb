{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "faqs = \"\"\"About the Program\n",
        "\n",
        "What is the Course fee for Data Science Mentorship Program (DSMP 2023)?\n",
        "The course follows a monthly subscription model where you have to make monthly payments of Rs 799/month.\n",
        "\n",
        "What is the total duration of the course?\n",
        "The total duration of the course is 7 months. So the total course fee becomes 799*7 = Rs 5600 (approx.)\n",
        "\n",
        "What is the syllabus of the mentorship program?\n",
        "We will be covering the following modules:\n",
        "- Python Fundamentals\n",
        "- Python libraries for Data Science\n",
        "- Data Analysis\n",
        "- SQL for Data Science\n",
        "- Maths for Machine Learning\n",
        "- ML Algorithms\n",
        "- Practical ML\n",
        "- MLOps\n",
        "- Case studies\n",
        "\n",
        "You can check the detailed syllabus here - https://learnwith.campusx.in/courses/CampusX-Data-Science-Mentorship-Program\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6GftujP4RAT8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts([faqs])"
      ],
      "metadata": {
        "id": "4RWtzrhGYBF5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgrdKzxNYiAR",
        "outputId": "3b2bd3e9-ed0b-4a77-d728-4d6436a018a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'course': 2,\n",
              " 'data': 3,\n",
              " 'program': 4,\n",
              " 'is': 5,\n",
              " 'for': 6,\n",
              " 'science': 7,\n",
              " 'of': 8,\n",
              " 'what': 9,\n",
              " 'mentorship': 10,\n",
              " 'total': 11,\n",
              " 'fee': 12,\n",
              " 'monthly': 13,\n",
              " 'you': 14,\n",
              " 'rs': 15,\n",
              " '799': 16,\n",
              " 'duration': 17,\n",
              " '7': 18,\n",
              " 'syllabus': 19,\n",
              " 'python': 20,\n",
              " 'ml': 21,\n",
              " 'campusx': 22,\n",
              " 'about': 23,\n",
              " 'dsmp': 24,\n",
              " '2023': 25,\n",
              " 'follows': 26,\n",
              " 'a': 27,\n",
              " 'subscription': 28,\n",
              " 'model': 29,\n",
              " 'where': 30,\n",
              " 'have': 31,\n",
              " 'to': 32,\n",
              " 'make': 33,\n",
              " 'payments': 34,\n",
              " 'month': 35,\n",
              " 'months': 36,\n",
              " 'so': 37,\n",
              " 'becomes': 38,\n",
              " '5600': 39,\n",
              " 'approx': 40,\n",
              " 'we': 41,\n",
              " 'will': 42,\n",
              " 'be': 43,\n",
              " 'covering': 44,\n",
              " 'following': 45,\n",
              " 'modules': 46,\n",
              " 'fundamentals': 47,\n",
              " 'libraries': 48,\n",
              " 'analysis': 49,\n",
              " 'sql': 50,\n",
              " 'maths': 51,\n",
              " 'machine': 52,\n",
              " 'learning': 53,\n",
              " 'algorithms': 54,\n",
              " 'practical': 55,\n",
              " 'mlops': 56,\n",
              " 'case': 57,\n",
              " 'studies': 58,\n",
              " 'can': 59,\n",
              " 'check': 60,\n",
              " 'detailed': 61,\n",
              " 'here': 62,\n",
              " 'https': 63,\n",
              " 'learnwith': 64,\n",
              " 'in': 65,\n",
              " 'courses': 66}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in faqs.split('\\n'):\n",
        "  t_sen=tokenizer.texts_to_sequences([sentence])[0]\n",
        "  print(t_sen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFJFERVQcnWy",
        "outputId": "11adf3f4-8b48-4125-bcbe-0a0ad36d76cb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[23, 1, 4]\n",
            "[]\n",
            "[9, 5, 1, 2, 12, 6, 3, 7, 10, 4, 24, 25]\n",
            "[1, 2, 26, 27, 13, 28, 29, 30, 14, 31, 32, 33, 13, 34, 8, 15, 16, 35]\n",
            "[]\n",
            "[9, 5, 1, 11, 17, 8, 1, 2]\n",
            "[1, 11, 17, 8, 1, 2, 5, 18, 36, 37, 1, 11, 2, 12, 38, 16, 18, 15, 39, 40]\n",
            "[]\n",
            "[9, 5, 1, 19, 8, 1, 10, 4]\n",
            "[41, 42, 43, 44, 1, 45, 46]\n",
            "[20, 47]\n",
            "[20, 48, 6, 3, 7]\n",
            "[3, 49]\n",
            "[50, 6, 3, 7]\n",
            "[51, 6, 52, 53]\n",
            "[21, 54]\n",
            "[55, 21]\n",
            "[56]\n",
            "[57, 58]\n",
            "[]\n",
            "[14, 59, 60, 1, 61, 19, 62, 63, 64, 22, 65, 66, 22, 3, 7, 10, 4]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences=[]\n",
        "for sentence in faqs.split('\\n'):\n",
        "  t_sen=tokenizer.texts_to_sequences([sentence])[0]\n",
        "  for i in range(1,len(t_sen)):\n",
        "    n_gram = t_sen[:i+1]\n",
        "    input_sequences.append(n_gram)\n",
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_0nc9rZYoyr",
        "outputId": "64560509-a43e-418a-d4b8-8e315bac028f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[23, 1],\n",
              " [23, 1, 4],\n",
              " [9, 5],\n",
              " [9, 5, 1],\n",
              " [9, 5, 1, 2],\n",
              " [9, 5, 1, 2, 12],\n",
              " [9, 5, 1, 2, 12, 6],\n",
              " [9, 5, 1, 2, 12, 6, 3],\n",
              " [9, 5, 1, 2, 12, 6, 3, 7],\n",
              " [9, 5, 1, 2, 12, 6, 3, 7, 10],\n",
              " [9, 5, 1, 2, 12, 6, 3, 7, 10, 4],\n",
              " [9, 5, 1, 2, 12, 6, 3, 7, 10, 4, 24],\n",
              " [9, 5, 1, 2, 12, 6, 3, 7, 10, 4, 24, 25],\n",
              " [1, 2],\n",
              " [1, 2, 26],\n",
              " [1, 2, 26, 27],\n",
              " [1, 2, 26, 27, 13],\n",
              " [1, 2, 26, 27, 13, 28],\n",
              " [1, 2, 26, 27, 13, 28, 29],\n",
              " [1, 2, 26, 27, 13, 28, 29, 30],\n",
              " [1, 2, 26, 27, 13, 28, 29, 30, 14],\n",
              " [1, 2, 26, 27, 13, 28, 29, 30, 14, 31],\n",
              " [1, 2, 26, 27, 13, 28, 29, 30, 14, 31, 32],\n",
              " [1, 2, 26, 27, 13, 28, 29, 30, 14, 31, 32, 33],\n",
              " [1, 2, 26, 27, 13, 28, 29, 30, 14, 31, 32, 33, 13],\n",
              " [1, 2, 26, 27, 13, 28, 29, 30, 14, 31, 32, 33, 13, 34],\n",
              " [1, 2, 26, 27, 13, 28, 29, 30, 14, 31, 32, 33, 13, 34, 8],\n",
              " [1, 2, 26, 27, 13, 28, 29, 30, 14, 31, 32, 33, 13, 34, 8, 15],\n",
              " [1, 2, 26, 27, 13, 28, 29, 30, 14, 31, 32, 33, 13, 34, 8, 15, 16],\n",
              " [1, 2, 26, 27, 13, 28, 29, 30, 14, 31, 32, 33, 13, 34, 8, 15, 16, 35],\n",
              " [9, 5],\n",
              " [9, 5, 1],\n",
              " [9, 5, 1, 11],\n",
              " [9, 5, 1, 11, 17],\n",
              " [9, 5, 1, 11, 17, 8],\n",
              " [9, 5, 1, 11, 17, 8, 1],\n",
              " [9, 5, 1, 11, 17, 8, 1, 2],\n",
              " [1, 11],\n",
              " [1, 11, 17],\n",
              " [1, 11, 17, 8],\n",
              " [1, 11, 17, 8, 1],\n",
              " [1, 11, 17, 8, 1, 2],\n",
              " [1, 11, 17, 8, 1, 2, 5],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18, 36],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18, 36, 37],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18, 36, 37, 1],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18, 36, 37, 1, 11],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18, 36, 37, 1, 11, 2],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18, 36, 37, 1, 11, 2, 12],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18, 36, 37, 1, 11, 2, 12, 38],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18, 36, 37, 1, 11, 2, 12, 38, 16],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18, 36, 37, 1, 11, 2, 12, 38, 16, 18],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18, 36, 37, 1, 11, 2, 12, 38, 16, 18, 15],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18, 36, 37, 1, 11, 2, 12, 38, 16, 18, 15, 39],\n",
              " [1, 11, 17, 8, 1, 2, 5, 18, 36, 37, 1, 11, 2, 12, 38, 16, 18, 15, 39, 40],\n",
              " [9, 5],\n",
              " [9, 5, 1],\n",
              " [9, 5, 1, 19],\n",
              " [9, 5, 1, 19, 8],\n",
              " [9, 5, 1, 19, 8, 1],\n",
              " [9, 5, 1, 19, 8, 1, 10],\n",
              " [9, 5, 1, 19, 8, 1, 10, 4],\n",
              " [41, 42],\n",
              " [41, 42, 43],\n",
              " [41, 42, 43, 44],\n",
              " [41, 42, 43, 44, 1],\n",
              " [41, 42, 43, 44, 1, 45],\n",
              " [41, 42, 43, 44, 1, 45, 46],\n",
              " [20, 47],\n",
              " [20, 48],\n",
              " [20, 48, 6],\n",
              " [20, 48, 6, 3],\n",
              " [20, 48, 6, 3, 7],\n",
              " [3, 49],\n",
              " [50, 6],\n",
              " [50, 6, 3],\n",
              " [50, 6, 3, 7],\n",
              " [51, 6],\n",
              " [51, 6, 52],\n",
              " [51, 6, 52, 53],\n",
              " [21, 54],\n",
              " [55, 21],\n",
              " [57, 58],\n",
              " [14, 59],\n",
              " [14, 59, 60],\n",
              " [14, 59, 60, 1],\n",
              " [14, 59, 60, 1, 61],\n",
              " [14, 59, 60, 1, 61, 19],\n",
              " [14, 59, 60, 1, 61, 19, 62],\n",
              " [14, 59, 60, 1, 61, 19, 62, 63],\n",
              " [14, 59, 60, 1, 61, 19, 62, 63, 64],\n",
              " [14, 59, 60, 1, 61, 19, 62, 63, 64, 22],\n",
              " [14, 59, 60, 1, 61, 19, 62, 63, 64, 22, 65],\n",
              " [14, 59, 60, 1, 61, 19, 62, 63, 64, 22, 65, 66],\n",
              " [14, 59, 60, 1, 61, 19, 62, 63, 64, 22, 65, 66, 22],\n",
              " [14, 59, 60, 1, 61, 19, 62, 63, 64, 22, 65, 66, 22, 3],\n",
              " [14, 59, 60, 1, 61, 19, 62, 63, 64, 22, 65, 66, 22, 3, 7],\n",
              " [14, 59, 60, 1, 61, 19, 62, 63, 64, 22, 65, 66, 22, 3, 7, 10],\n",
              " [14, 59, 60, 1, 61, 19, 62, 63, 64, 22, 65, 66, 22, 3, 7, 10, 4]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=max([len(x) for x in input_sequences])\n",
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pTT2RDBZKuM",
        "outputId": "8446f798-7a4c-4b76-c03f-7fd2d06b4f6c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import pad_sequences\n",
        "padded_seq=pad_sequences(input_sequences,padding='pre',maxlen=max_len)\n",
        "padded_seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eps646HGgR58",
        "outputId": "18e2a1d5-667f-4d59-daab-e4994ce90730"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0, ...,  0, 23,  1],\n",
              "       [ 0,  0,  0, ..., 23,  1,  4],\n",
              "       [ 0,  0,  0, ...,  0,  9,  5],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 22,  3,  7],\n",
              "       [ 0,  0,  0, ...,  3,  7, 10],\n",
              "       [ 0,  0,  0, ...,  7, 10,  4]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=padded_seq[:,:-1]\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zztyqs1iwSQ",
        "outputId": "72981ad2-ae37-42f8-cc6f-42a186115a17"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0, ...,  0,  0, 23],\n",
              "       [ 0,  0,  0, ...,  0, 23,  1],\n",
              "       [ 0,  0,  0, ...,  0,  0,  9],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ..., 66, 22,  3],\n",
              "       [ 0,  0,  0, ..., 22,  3,  7],\n",
              "       [ 0,  0,  0, ...,  3,  7, 10]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=padded_seq[:,-1]\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFMybSyHj7LA",
        "outputId": "9aa8eb87-115c-4cbf-c48d-15697d1dcde1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  4,  5,  1,  2, 12,  6,  3,  7, 10,  4, 24, 25,  2, 26, 27, 13,\n",
              "       28, 29, 30, 14, 31, 32, 33, 13, 34,  8, 15, 16, 35,  5,  1, 11, 17,\n",
              "        8,  1,  2, 11, 17,  8,  1,  2,  5, 18, 36, 37,  1, 11,  2, 12, 38,\n",
              "       16, 18, 15, 39, 40,  5,  1, 19,  8,  1, 10,  4, 42, 43, 44,  1, 45,\n",
              "       46, 47, 48,  6,  3,  7, 49,  6,  3,  7,  6, 52, 53, 54, 21, 58, 59,\n",
              "       60,  1, 61, 19, 62, 63, 64, 22, 65, 66, 22,  3,  7, 10,  4],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW2ryHf_kGJk",
        "outputId": "4bf4d808-b000-4a8f-fdd5-39fd62b3d0e8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkSc7Dd2nWsS",
        "outputId": "62dd55d1-568c-4450-af54-c9c8f222c79c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoePBxWvnwoR",
        "outputId": "d02e6ac3-8d6c-4c60-d763-1dd1c6636d09"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y=to_categorical(y,num_classes=67)"
      ],
      "metadata": {
        "id": "4OFmgVWfnaBu"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiWB1hswoPqF",
        "outputId": "a046e356-44c1-42df-8cc8-76761d114ff3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 67)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A9aUW8YoTEU",
        "outputId": "546ce314-864b-4360-aad7-a8f2410ee6d6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "YMk1d64_p-Ls"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(282,100,input_length=19)) #vocabulary count,output dense vector, input length\n",
        "model.add(LSTM(150)) # 150 nodes\n",
        "model.add(Dense(67,activation='softmax')) # vocabulary count\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) # because it is a multiclass classification problem.\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-3Z_9iirNo0",
        "outputId": "0c6aa84f-2cd4-48a5-815c-c99b91d8e367"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 19, 100)           28200     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 150)               150600    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 67)                10117     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 188917 (737.96 KB)\n",
            "Trainable params: 188917 (737.96 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding layer convert the sparse vector into dense vector (1,56,100) for each word, there are 100 numbers."
      ],
      "metadata": {
        "id": "s-cwnNR5sts6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x,y,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyWF5Ov8tqGy",
        "outputId": "750cfe8e-9be2-407f-be6e-add5ad6cef34"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 3s 38ms/step - loss: 4.2028 - accuracy: 0.0200\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 4.1741 - accuracy: 0.0800\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 4.1327 - accuracy: 0.1100\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 4.0154 - accuracy: 0.1000\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 3.9076 - accuracy: 0.1000\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 3.8513 - accuracy: 0.1000\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 3.8152 - accuracy: 0.1100\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 3.7928 - accuracy: 0.1200\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 3.7692 - accuracy: 0.1000\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 3.7406 - accuracy: 0.1100\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 3.7163 - accuracy: 0.1200\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 3.7005 - accuracy: 0.1100\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 3.6826 - accuracy: 0.1200\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 63ms/step - loss: 3.6573 - accuracy: 0.1300\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 3.6223 - accuracy: 0.1500\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 3.6021 - accuracy: 0.1600\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 3.5824 - accuracy: 0.1500\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 3.5394 - accuracy: 0.1300\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 62ms/step - loss: 3.4803 - accuracy: 0.1300\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 3.4671 - accuracy: 0.1400\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 3.4141 - accuracy: 0.1200\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 3.3252 - accuracy: 0.1300\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 65ms/step - loss: 3.2805 - accuracy: 0.1900\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 3.1974 - accuracy: 0.2000\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 3.1310 - accuracy: 0.2100\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 3.0553 - accuracy: 0.3000\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 2.9825 - accuracy: 0.2700\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 62ms/step - loss: 2.8897 - accuracy: 0.2700\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 2.8291 - accuracy: 0.2800\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 2.7618 - accuracy: 0.2700\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 2.6615 - accuracy: 0.2500\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 2.5996 - accuracy: 0.2700\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 2.5364 - accuracy: 0.3000\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 2.4633 - accuracy: 0.3500\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 2.4201 - accuracy: 0.3700\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 2.3435 - accuracy: 0.4000\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 2.2993 - accuracy: 0.3600\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 2.2324 - accuracy: 0.4000\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 2.1659 - accuracy: 0.4200\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 2.1094 - accuracy: 0.4100\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 2.1143 - accuracy: 0.3800\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 2.1154 - accuracy: 0.3600\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 2.0388 - accuracy: 0.4000\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.9639 - accuracy: 0.4200\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1.9255 - accuracy: 0.4000\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1.8570 - accuracy: 0.5000\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1.8129 - accuracy: 0.5200\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1.7651 - accuracy: 0.5200\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1.7128 - accuracy: 0.5000\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1.6579 - accuracy: 0.5500\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1.6392 - accuracy: 0.5600\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1.5880 - accuracy: 0.5800\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.5538 - accuracy: 0.5900\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.5065 - accuracy: 0.5900\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1.4679 - accuracy: 0.6100\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.4478 - accuracy: 0.6200\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1.4284 - accuracy: 0.6600\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1.3681 - accuracy: 0.6500\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1.3219 - accuracy: 0.6600\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.2954 - accuracy: 0.7100\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1.2441 - accuracy: 0.7200\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1.2169 - accuracy: 0.7200\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1.2159 - accuracy: 0.6900\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1.1840 - accuracy: 0.7000\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1.1584 - accuracy: 0.7000\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1.1441 - accuracy: 0.6500\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1.0846 - accuracy: 0.7100\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1.0483 - accuracy: 0.7200\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 1.0181 - accuracy: 0.6900\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.9897 - accuracy: 0.7600\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.9776 - accuracy: 0.7700\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.9518 - accuracy: 0.8100\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.9139 - accuracy: 0.8600\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.8902 - accuracy: 0.8200\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.8723 - accuracy: 0.8300\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.8420 - accuracy: 0.8600\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.8133 - accuracy: 0.8500\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.7881 - accuracy: 0.8900\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.7752 - accuracy: 0.8900\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 0.7542 - accuracy: 0.8700\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.7378 - accuracy: 0.9000\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.7126 - accuracy: 0.9100\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.6895 - accuracy: 0.9000\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.6701 - accuracy: 0.9100\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.6663 - accuracy: 0.8900\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.6467 - accuracy: 0.9100\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.6333 - accuracy: 0.9200\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.6147 - accuracy: 0.9200\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.6007 - accuracy: 0.9300\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.5970 - accuracy: 0.9300\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.5736 - accuracy: 0.9300\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.5746 - accuracy: 0.9100\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.5429 - accuracy: 0.9400\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.5519 - accuracy: 0.9300\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.5284 - accuracy: 0.9400\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.5373 - accuracy: 0.9300\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.5422 - accuracy: 0.9200\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.4995 - accuracy: 0.9500\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.5172 - accuracy: 0.9300\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.4754 - accuracy: 0.9400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dd666883b20>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text='Maths'\n",
        "# tokenize\n",
        "token_text=tokenizer.texts_to_sequences([text])[0]\n",
        "# padding\n",
        "padded_token=pad_sequences([token_text],maxlen=19,padding='pre')\n",
        "# predict\n",
        "model.predict(padded_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO0__L2fxAbC",
        "outputId": "9b5ec7c7-eff0-4638-a7ef-52bfb1c9703c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 449ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.82764517e-05, 2.44229613e-03, 1.81609695e-03, 3.14449333e-03,\n",
              "        3.29887669e-04, 1.31301116e-02, 7.44202554e-01, 7.81112525e-04,\n",
              "        1.39626488e-03, 3.08689778e-05, 3.37988633e-04, 5.35674300e-03,\n",
              "        3.69828753e-03, 2.55865254e-03, 1.15609575e-04, 2.45037263e-05,\n",
              "        1.27898093e-04, 1.13927817e-03, 2.18945512e-04, 3.14498367e-03,\n",
              "        4.95488639e-05, 1.82369798e-02, 6.92183501e-04, 2.61334317e-05,\n",
              "        4.95770801e-05, 3.09929892e-05, 1.09492242e-02, 9.43179335e-03,\n",
              "        3.81788966e-04, 1.53394751e-04, 9.17295038e-05, 2.03889009e-04,\n",
              "        9.20049497e-05, 4.93283005e-05, 8.36373511e-05, 2.36708092e-06,\n",
              "        8.31615689e-05, 3.08626535e-04, 1.84292745e-04, 6.31807552e-06,\n",
              "        3.86076272e-06, 2.89803374e-05, 8.11336748e-03, 1.74242246e-03,\n",
              "        2.25501601e-03, 3.62902414e-04, 3.30197247e-04, 2.51853820e-02,\n",
              "        2.12956350e-02, 9.57928505e-03, 3.49630609e-05, 4.33399473e-05,\n",
              "        1.07852668e-02, 5.95092960e-03, 2.55926698e-02, 4.67533646e-05,\n",
              "        5.42520283e-05, 2.09432455e-05, 2.97282618e-02, 2.40663365e-02,\n",
              "        6.08618092e-03, 1.36544521e-03, 7.00362783e-04, 3.08573537e-04,\n",
              "        2.39698740e-04, 3.97208059e-04, 5.30098623e-04]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "pos=np.argmax(model.predict(padded_token))\n",
        "for word,index in tokenizer.word_index.items():\n",
        "  if index==pos:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzTQPR7zy4JJ",
        "outputId": "c92d6a09-6ba5-4aa6-d887-2ed23bb06f49"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 41ms/step\n",
            "for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-JwZ-hg0cp0",
        "outputId": "ded23a2b-b228-4fe9-b494-ea80133e24b7"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'course': 2,\n",
              " 'data': 3,\n",
              " 'program': 4,\n",
              " 'is': 5,\n",
              " 'for': 6,\n",
              " 'science': 7,\n",
              " 'of': 8,\n",
              " 'what': 9,\n",
              " 'mentorship': 10,\n",
              " 'total': 11,\n",
              " 'fee': 12,\n",
              " 'monthly': 13,\n",
              " 'you': 14,\n",
              " 'rs': 15,\n",
              " '799': 16,\n",
              " 'duration': 17,\n",
              " '7': 18,\n",
              " 'syllabus': 19,\n",
              " 'python': 20,\n",
              " 'ml': 21,\n",
              " 'campusx': 22,\n",
              " 'about': 23,\n",
              " 'dsmp': 24,\n",
              " '2023': 25,\n",
              " 'follows': 26,\n",
              " 'a': 27,\n",
              " 'subscription': 28,\n",
              " 'model': 29,\n",
              " 'where': 30,\n",
              " 'have': 31,\n",
              " 'to': 32,\n",
              " 'make': 33,\n",
              " 'payments': 34,\n",
              " 'month': 35,\n",
              " 'months': 36,\n",
              " 'so': 37,\n",
              " 'becomes': 38,\n",
              " '5600': 39,\n",
              " 'approx': 40,\n",
              " 'we': 41,\n",
              " 'will': 42,\n",
              " 'be': 43,\n",
              " 'covering': 44,\n",
              " 'following': 45,\n",
              " 'modules': 46,\n",
              " 'fundamentals': 47,\n",
              " 'libraries': 48,\n",
              " 'analysis': 49,\n",
              " 'sql': 50,\n",
              " 'maths': 51,\n",
              " 'machine': 52,\n",
              " 'learning': 53,\n",
              " 'algorithms': 54,\n",
              " 'practical': 55,\n",
              " 'mlops': 56,\n",
              " 'case': 57,\n",
              " 'studies': 58,\n",
              " 'can': 59,\n",
              " 'check': 60,\n",
              " 'detailed': 61,\n",
              " 'here': 62,\n",
              " 'https': 63,\n",
              " 'learnwith': 64,\n",
              " 'in': 65,\n",
              " 'courses': 66}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text='Maths'\n",
        "for i in range(3):\n",
        "  # tokenize\n",
        "  token_text=tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token=pad_sequences([token_text],maxlen=19,padding='pre')\n",
        "  # predict\n",
        "  model.predict(padded_token)\n",
        "  pos=np.argmax(model.predict(padded_token))\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index==pos:\n",
        "      text=text+' '+word\n",
        "      print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "490bG2Mq0mTA",
        "outputId": "1664acda-e3c1-4c58-bb40-64808ee56b59"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Maths for\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Maths for machine\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Maths for machine learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text='we are going'\n",
        "for i in range(10):\n",
        "  # tokenize\n",
        "  token_text=tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token=pad_sequences([token_text],maxlen=19,padding='pre')\n",
        "  # predict\n",
        "  model.predict(padded_token)\n",
        "  pos=np.argmax(model.predict(padded_token))\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index==pos:\n",
        "      text=text+' '+word\n",
        "      print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKbqGHAf1_Eq",
        "outputId": "0601e491-8bdb-4ff0-8789-3fa1aef7d01b"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "we are going will\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "we are going will be\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "we are going will be covering\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "we are going will be covering the\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "we are going will be covering the following\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "we are going will be covering the following modules\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "we are going will be covering the following modules modules\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "we are going will be covering the following modules modules modules\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "we are going will be covering the following modules modules modules learnwith\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "we are going will be covering the following modules modules modules learnwith you\n"
          ]
        }
      ]
    }
  ]
}